{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to create the filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data and tokenizing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting ngrams...\n",
      "((29901, 13), (13, 2277), (29889, 13), (25580, 29962), (13, 29961), (14350, 263), (29962, 13), (7128, 2486), (263, 2933), (1614, 2167), (2167, 278), (2933, 393), (278, 2009), (13291, 29901), (2799, 4080), (4080, 29901), (518, 25580), (393, 7128), (1, 518), (29962, 14350), (2486, 1614), (2277, 2799), (29961, 29914), (29914, 25580), (2277, 13291), (3030, 29889), (278, 3030), (2183, 278), (15228, 29901), (2009, 2183), (2277, 15228), (29889, 1), (29892, 322), (29915, 29879), (2009, 29889), (13, 1576), (13, 29908), (243, 162), (13, 13), (310, 278), (13, 5631), (5631, 403), (297, 278), (363, 263), (29871, 29896), (29889, 450), (29871, 243), (403, 385), (13, 29909), (373, 278), (13, 1184), (1184, 29894), (29894, 680), (310, 263), (29892, 13), (29973, 13), (297, 263), (304, 278), (680, 263), (29871, 29906), (338, 263), (13, 29954), (6113, 263), (13, 6113), (29892, 278), (411, 263), (304, 263), (13, 29896), (1213, 1), (363, 278), (29892, 306), (13, 29902), (29908, 1), (13, 4002), (4002, 29581), (907, 1230), (29915, 29873), (29889, 739), (29954, 573), (13, 29899), (263, 716), (29892, 263), (403, 263), (29871, 29941), (29896, 29900), (29892, 541), (29896, 29889), (29889, 306), (29991, 29871), (29906, 29889), (319, 29902), (162, 155), (322, 278), (13, 29906), (29941, 29889), (2729, 373), (306, 29915), (373, 263), (29900, 29900), (13, 29941))\n",
      "(434131, 419693, 396257, 326218, 166166, 164579, 164506, 164336, 163591, 163265, 163230, 163160, 163146, 163134, 163125, 163112, 163111, 163110, 163109, 163109, 163109, 163109, 163109, 163109, 163109, 93948, 93799, 93749, 93509, 93475, 93475, 79447, 76526, 74700, 69873, 61280, 50404, 50361, 44767, 39595, 38694, 38671, 36999, 33781, 29827, 27558, 27041, 25648, 25399, 23019, 22915, 22716, 22701, 21810, 21631, 21063, 20396, 20166, 19276, 18267, 18046, 17461, 16762, 16716, 16706, 16505, 16459, 16049, 15777, 15649, 15513, 15311, 15306, 15202, 14897, 14752, 14534, 14126, 14071, 13616, 13608, 13563, 13382, 13370, 13014, 12658, 12623, 12452, 12404, 12398, 12387, 11956, 11903, 11761, 11645, 11623, 11349, 10930, 10921, 10889)\n"
     ]
    }
   ],
   "source": [
    "path_to_data = \"data/maryland_ngram2_seed3.jsonl\" # data from which to compute the filter\n",
    "path_to_model =  \"\" # path to model to use for tokenization\n",
    "path_to_filter = \".\" # where to save the filter labels_values.pkl\n",
    "window_size = 2 # n-gram size for the filter computation\n",
    "\n",
    "def count_ngrams(toks, n):\n",
    "    # Generate n-grams\n",
    "    n_grams = list(ngrams(toks, n))\n",
    "    # Count the frequency of each n-gram\n",
    "    n_gram_counts = Counter(n_grams)\n",
    "    return n_gram_counts\n",
    "\n",
    "def parallel_count_ngrams(toks, n, num_processes):\n",
    "    # Split the toks into chunks for each process\n",
    "    chunks = [toks[ii::num_processes] for ii in range(num_processes)]\n",
    "    # Create a pool of processes\n",
    "    with mp.Pool(processes=num_processes) as pool:\n",
    "        # Use starmap to apply the count_ngrams function to each chunk\n",
    "        results = pool.starmap(count_ngrams, [(chunk, n) for chunk in chunks])\n",
    "    # Combine the results from each process\n",
    "    combined_counts = sum(results, Counter())\n",
    "    return combined_counts\n",
    "\n",
    "def process_file(args, mp_mode=False, prop=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        args: (jsonl_path, nn, save_dir)\n",
    "        mp_mode: whether to use multiprocessing\n",
    "    Returns:\n",
    "        labels: list of ngrams\n",
    "        values: list of counts for each ngram\n",
    "    \"\"\"\n",
    "    jsonl_path, nn, save_dir = args\n",
    "    print(\"loading data and tokenizing...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
    "    result_key = \"text\"\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "        taille = len(data)\n",
    "        new_length = prop*taille\n",
    "        data = [item[\"input\"] + item[\"output\"] for i,item in enumerate(data) if i<=new_length]\n",
    "\n",
    "    toks = []\n",
    "    for item in data:\n",
    "        toks.extend(tokenizer.encode(item))\n",
    "\n",
    "    print(\"counting ngrams...\")\n",
    "    if mp_mode:\n",
    "        n_gram_counts = parallel_count_ngrams(toks, nn, 4)\n",
    "    else:\n",
    "        # Generate n-grams\n",
    "        n_grams = list(ngrams(toks, nn))\n",
    "        # Count the frequency of each n-gram\n",
    "        n_gram_counts = Counter(n_grams)\n",
    "    labels, values = zip(*n_gram_counts.items())\n",
    "\n",
    "    # order by frequency\n",
    "    labels, values = zip(*sorted(zip(labels, values), key=lambda item: item[1], reverse=True))\n",
    "    print(labels[:100])\n",
    "    print(values[:100])\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    # save labels and values\n",
    "    save_path = os.path.join(save_dir, \"filter.pkl\")\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump((labels, values), f)\n",
    "    \n",
    "    return labels, values\n",
    "\n",
    "\n",
    "# Saves a pkl that contains the ngrams and their counts\n",
    "labels, values = process_file((path_to_data, window_size, path_to_filter), mp_mode=False, prop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2416_radioactive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
